## ğŸ§  MultiAgent Vision Intelligence (MAVI)

A modular, multi-agent AI system that uses **LLaVA (Large Language and Vision Assistant)** to process images for **semantic understanding** and **similarity-based retrieval**. This project combines the strengths of vision-language models with autonomous agents to create a scalable, intelligent annotation and search engine.

---

## ğŸš€ Key Features

- ğŸ” **Semantic Search**: Understand and query images using natural language prompts.
- ğŸ”— **Similarity Search**: Retrieve images with similar visual or contextual features.
- ğŸ¤– **Multi-Agent Architecture**: Independent agents handle tasks like embedding, querying, indexing, and reasoning.
- ğŸ§© **Pluggable Agents**: Easily swap models or services (e.g., LLaVA, FAISS, OpenAI, etc.)
- ğŸ–¼ï¸ **Vision-Language Integration**: Powered by LLaVA for interpreting image content using LLMs.

---

## ğŸ§¬ System Architecture

![System Architecture](images/architecture.png)


## ğŸ§± Components

| Agent             | Description                                                                 |
|------------------|-----------------------------------------------------------------------------|
| `SemanticAgent`   | Uses LLaVA to perform reasoning and image captioning.                       |
| `SimilarityAgent` | Embeds input and finds similar items using vector DBs like FAISS or Qdrant. |
| `Controller`      | Routes user queries to the appropriate agent(s).                            |
| `Visualizer`      | Optional â€” overlays results, annotations, or outputs.                       |

---

## ğŸ›  Installation

### 1. Clone the repository
```bash
git clone https://github.com/your-username/multiagent-vision-intelligence.git
cd multiagent-vision-intelligence
````

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Install Ollama and Pull LLaVA Model

> â„¹ï¸ [Ollama installation guide](https://ollama.com/download)

```bash
# Install Ollama if not already installed
# Then run:
ollama pull llava:latest
ollama serve
```

Make sure Ollama is running at `http://localhost:11434`.

---

## ğŸ§ª Usage

```bash
uvicorn main:app --reload
```

Then:

* ğŸ“¤ Upload an image or send a text prompt
* ğŸ” Use endpoints like:

```
POST /search/semantic
POST /search/similarity
```

---

## ğŸ“¦ Technologies

* **FastAPI** â€“ REST API backend
* **LLaVA** â€“ Vision-language model served locally via Ollama
* **ChromaDB** â€“ Similarity search engine
* **NumPy, OpenCV, PIL** â€“ Image processing
* **AutoGen Agents** â€“ Multi-agent orchestration

---

## ğŸ“ Citation / Attribution

Please cite or credit this repository if used in your work.
See [LICENSE](LICENSE) for terms.

```bibtex
@misc{kameshwaran2025multiagent,
  author = {Kameshwaran S.},
  title = {MultiAgent Vision Intelligence},
  year = {2025},
  url = {https://github.com/Kameshwaran-45/multiagent-vision-intelligence}
}
```

---

## ğŸ¤ Contributing

Contributions, ideas, and improvements are welcome!
Please open an issue or submit a pull request.
